{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd075ff616eabfbb04cd70365352177948bcdc5db545828d387866d72aa5ede8861",
   "display_name": "Python 3.9.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-8454d5381422>:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  df = csv.groupby(['Country_Region'])['Confirmed', 'Deaths', 'Recovered'].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "df = csv.groupby(['Country_Region'])['Confirmed', 'Deaths', 'Recovered'].sum().reset_index()\r\n",
    "df = df[[\"Country_Region\", \"Confirmed\", \"Deaths\", \"Recovered\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = df.copy()\n",
    "# df_scaled['Confirmed'].plot(kind = 'bar')\n",
    "df_scaled['Confirmed'] = (df_scaled['Confirmed'] - df_scaled['Confirmed'].min()) / (df_scaled['Confirmed'].max() - df_scaled['Confirmed'].min())\n",
    "df_scaled['Deaths'] = (df_scaled['Deaths'] - df_scaled['Deaths'].min()) / (df_scaled['Deaths'].max() - df_scaled['Deaths'].min())\n",
    "df_scaled['Recovered'] = (df_scaled['Recovered'] - df_scaled['Recovered'].min()) / (df_scaled['Recovered'].max() - df_scaled['Recovered'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_dim = (10, 10) # x, y dimensions\n",
    "learning_rate = 0.05\n",
    "iterations = 1000\n",
    "\n",
    "radius = max(som_dim) // 2\n",
    "data_dimensions = df_scaled.shape[1] - 1 #remove index from shape\n",
    "time_const = iterations / math.log(radius)\n",
    "\n",
    "neurons = [[] for _ in range(som_dim[0])]\n",
    "\n",
    "for row in range(som_dim[0]):\n",
    "    for col in range(som_dim[1]):\n",
    "        neurons[row].append([random.random() for _ in range(data_dimensions)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "((5, 6), 0.12248235268433319)\n"
     ]
    }
   ],
   "source": [
    "for m_iter in range(iterations):\n",
    "    input_vec = df_scaled.sample()\n",
    "    input_vec = input_vec.values.tolist()[0][1:]\n",
    "    \n",
    "    dist = [] #list of tuples ( (i,j), distance )\n",
    "\n",
    "    for row in range(som_dim[0]):\n",
    "        for col in range(som_dim[1]):\n",
    "            curr_neuron = neurons[row][col]\n",
    "            m_dist = math.dist(input_vec, curr_neuron)\n",
    "            dist.append( ((row, col),m_dist))\n",
    "    \n",
    "    neuron_ij, dist = min(dist, key=lambda x: x[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}